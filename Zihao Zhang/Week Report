2019/03/20
1.Finish Tanji3 DLA documents writing, include control module of LPE, control module of APE and the top module of control. The documents will be uploaded.
2.Study base 2 FFT algorithm that extracted by frequncy and base 4 FFT algorithm that extracted by time(base 4 FFT algorithm further reduce the computation complexity compared with base 2 FFT algorithm) in order to have a deeper understanding on Tingshua's Paper on ISSCC2019.
  a.Based on CirCNN
  b.Use FFT to compute convolution in CirCNN model
3.Read ISSCC2019 Paper "An 11.5TOPS/W 1024-MAC BUtterfly Structure Dual-Core Sparsity-Aware Neural Processing Unit in 8nm Flagship Mobile Soc":
  a.Moving Output Feature Maps to compute convolution
  b.Butterfly-Structure Dual-Core Accelerator
  c.Sparsity-Aware Computing:Using Feature Selection
4.Prepare the slide about accelerator based on Winograd Convolution.The Slides Will be uploaded.
2019/03/28
1.Read Paper "FPGA based efficient on-chip memory for image processing algorithms" to learn several main data access policies for image processing algorithms.
  a.Prposed a sub-bank Dual Port memory architecture based on Single Port SRAM which supports several main data access policies.
  b.Confused with its clock generation block and memory control unit.
2.Read Tingshua's Paper on ISSCC 2019, got its main idea about dataflow and methods to  efficiently do RFFT(The inputs are real numbers):
  a.Use CirCNN model proposed on Micro 2017.
  b.Using radix-2 DIT(decimation in time) algorithm and use 7-stage butterfly architecture and each stage contains 32 basic butterfly units to support up to 128-point FFT.
  c.Using "doubling algorithm" to avoid redundant operations in CFFT computing.
  But there are some details that I don't understand clearly:
  a.Why the weight matrixes are block-circulant matrixes after rearranging?
  b.Details about how to realize 1-to-12b data-width supporting.
  c.Details about HBST-TRAM.
  I will do a representation when I completely understand this paper.
3.Read Paper "A Pipelined FFT Architecture for Real-Valued Signals" to learn some architectures for real-valued signals.
4.Modify the slides of "Winograd Convolution", the representation will da tonight.
2019/04/04
1.Attend a lecture about accelerating ddep convolutional network inferences at algorithm level given by Bei Yu from CUHK on Friday afternoon.
2.Scan KAIST's ISSCC Paper about LNPU, an atchitecture aiming to accelerate DNN's training at edge. Need to study knowledge about training for further understanding.
  a.Use mixed precision(FP8 and FP16) to improve enegy efficiency with slight accuracy loss.
  b.Support zero-skipping operation in computing core.
3.Learn speech given by Bert Moons on ISSCC 2019, the topic is "Enabling Embedded Intelligence: Application, architecture and design solutions".
4.Ask for leave from Tuesday to Thursday.
2019/04/11
1.Study and understand some details of Tingshua's ISSCC2019 Paper which confused me during previous reading:
  a.Understand the method which makes the weight matrix after reformulation still be a block-circulant matrix
  b.The data-resue pattern during element-wise multiplication
  c.Serial-to-parallel transformation during dataflow
2.Find some new details that don't appear in slides and paper
  a.Because some twiddle factors can't be represented precisely，so FFT may bring error.
  b.The method of rounding between stages.
3.Write a block to do 64-point fft using radix-2 DIT algorithm for further research and modification. Not complete.
2019/04/18
1.Read paper "CircConv: A Structured Convolution with Low Complexity":
  a.Use circulant tensor for weight(the same as CirCNN does I think but use another method to explain the structured tensor)
  b.Propose a method to training circulant CNN from a pre-trained model
2.Read paper "Optimizing Bit-Serial Matrix Multiplication for Reconfigurable Computing":
  a.Intoduce a framework named "BISMO" to dp bit-serial matrix multiplication in order to support variable-precision computation proposed in their previous work
  b.Modify original Dot Product Unit(DPU) with matirx compression techniques to decrease the height of the input data at the cost of width increation.
  c.Modify the shifter in DPU by change the computation order so that the new shifter only need to support one type(shift left by one bit) of shift opertion
  d.Proposed bit-parallel to bit-serial transformaiton to reshape the data stored in main memory for efficient access.
  e.The software of "BISMO" framework still need to study.
3.Study math about matrix computation to find some tecniques to accelerate matrix computation(use the book writen by Gene H.Golub and Charles F.Van Loan)
2019/04/25
1.Think how to eliminate redundant operations during transformation of input feature map tile in Winograd convolution.
  a.Because there exists overlapping between neighboring input feature map tiles
2.Think how to use an unified computing architecture to support both complex-number matrix multiplication and real-number matrix multiplication.
3.Think if there exists an unified architecture supporting both radix-2 FFT and radix-4 FFT
2019/05/09
1.Read Paper about PERMDNN, which uses structured matrix-based approach like CirCNN to bring regularity to sparse  weight matrix.
  a.Make the weight matrices of the DNN model consist of multiple permuted diagnal sub-matrices.
  b.Permuted diagnal matrix require less storage than unstructured matrix becasue its its row index can be got by its column index.
  c.More flexibility than CirCNN because PERMCNN dose not have any restrictions on the size of block permuted diagnal matrix.
  d.Avoid computation between complex numbers
2.Read Paper about how to pack sparse CNNs for efficient systolic array implementations
  a.Use column combining to make sparse weight matrix become dense.
  b.Don't know how to combine the correspondingrows of input feature map matrix.
  c.Use bit-serial MAC.
2019/05/16
1.Read paper "An N-Way Group Association Architecture and Sparse Data Group Association Load Balancing Algorithm for Sparse CNN Accelerators" by the team of STICKER.
2.Think about if it is effective to store weights as Booth code and use Booth multiplication during computation of CNNs.
2019/05/25
Refer to Month Report-May.docx
2019/05/30
1.Read Paper "Architecture of a programmable systolic array"
  a.Streaming instructions like data
2.Think how to transfer pooling operation  to Winograd domain
2019/06/13
1.Read some papers about architectures of DNNs and types of convolution:
  a.HetConv: use heterogeneous convolution to replace homogeneous convolution, heterogeneous kernel means the size of weight matrices are different through input channels.
  b.DetNet: a backbone network for object detection based-on ResNet.
  c:Dialated covolution: use dialated convolution to replace pooling operation in order to avoid loss of spatial resolution.
2.Read paper "Partial-product Generation and Addition for Multiplication in FPGAs With 6-Input LUTs":
  a.Propose a booth architecture according to the features of CLB in Xilinx FPGA.
3.Read paper of DAC'2019:
  a.bitblade: modified the computation sequence of Bitfusion and acquire area and power reduction.
  b.Layer conscious memory management: solve the data transfer bottlenecks in FPGA-based DNN accelerator according to the shape of concrete layer.
2019/06/13
1.Do the work of assistant.
2.Think an idea about new DLA that sharing the partial production generation component of Booth multiplier during massive MAC operations.
2019/06/27
1.Prepare for the medium term report.
2.Do the work of assistant.
2019/07/04
Previous work this year:
1.Help analog group modify and verify a module.
2.Read papers about network architecture and DNN accelerator.
3.Learn FFT and study its combination with convolution computing.
4.Try to find some sparse matrices which are different from CirCNN and PermDNN.
Next Plan:
1.Study network training for future work.
2.Design a deep learning accelerator which supports 2-bit, 4-bit, 8-bit weight and 8-bit weight.
3.Complete the paper.
2019/07/11
1.Learn some light network architectures such as MobileNets and ShuffleNets. 
2.Think how to efficiently support some novel convolution architectures such as depthwise seperable convolution and so on.
  a.Read papers such as Eyeriss_v2 and MAERI(ASPLOS'18) which support flexible dataflow.
3.Think the soc design of the system for the final demo and discuss some details such as data transfer with TaoFeng.
2019/08/08
1.Talked with professor.
2.Took part in Chisel Community Conference 2019 on Saturday and Sunday to learning something about Chisel, a kind of HCL.
3.Learn someting about AXI4 Stream protocol and corresponding IP provided by Xilinx.
4.Think some detalis in paper "Deep Convolution Neural Network Architecture With Reconfigurable Computation Patterns"
To do list:
1.Continue working with the new idea, think how to reduce the resource consumption for encoding and partial product producing.
2.Continue learn something about network training.
3.Read paper that received from professor. Learn something about BERT.
2019/08/15
1.Help Jingsen debug the demo.
2.Study netwok traing based on MXNet.
3.Have an idea about pre-computation.(some additions can be completed before do inference on the chip).
2019/08/22
1.Learn feature extraction in Speech recognition and Synthesis.
2.Study details of Wavenet.
3.Train a very simple network for multi-classification using mxnet.
2019/08/29
1.Read code about implementation of Wavenet.
2.Read paper how to decrease the time complexity of computation in Wavenet according to pre-computation.
3.Learn some basic knowledge about general audio related tasks.
2019/08/29
1.Continue study details of original wavenet and fast wavenet.
2.Study how to use the DMA in Zynq Processing System.
2019/09/11
1.Finished code reading and made slides to illustrate the details about wavenet. 
2019/0925
1.Participated "The Industrial Fair" on Friday and Saturday.
2.Discussed with professor on Sunday.
3.Raed paper ”Rethinking floating point for deep learning” and related literature
2019/10/17
1.Completed the train of Tacotron2 and run some cases on trained Tacotron2 followed by official Waveglow.
2.made some graphs to show the data distribution of mel-spectrum generated by Tacotron2 and audio signal generated by official Waveglow.
3.Began tp train Waveglow using 4 GPUs.
4.Did some research about multi-operands adder.
2019/10/23
1.Read TingsHua's paper about accelerator about speech synthesis.
2.Think the architecture of the adder used to add a lot of fractions from posit floating point numbers togather.
2019/10/30
1.Writing patent.
2.Read paper about adders.
2019/11/06
1.Modify the patent.
2.Write 256-bit Carry Skip Adder according to a paper.
2019/11/13
1.Finished the Carry skip Addder writing and did some simply analysis and synthesis
2.Raed the paper about adder of Berkely and related papers.
To do list:
1.Use custom disign methodology to construct an adder.
2019/11/20
1.Study Ling's addition algorithm.
2.Study some tree architecture like kogge-stone and Ladner–Fischer, both dense and sparse.
2.Review "Digital Integrated Circuits".
2019/11/27
1.Design some basic gate circuits in virtusuo.
2.Learning some classical custom adder on MOSFET level.
2019/12/04
1.Simulated domino AND gate, domino OR gate, domino carry generation module and modified their size in order to get better result.
2.Construct sum precomputation logic and sum selection logic using standard cells in tsmc65lp.
3.Construct all stages of Kogge-Stone radix-2 sparse-2 64-bit adder(7 stages in total).
2019/12/11
1.Notes: onenote:https://d.docs.live.net/15b55244ee6801c7/文档/逸正%20的笔记本/Sequencial%20Generative%20Model.one#section-id={0C716B27-F959-4C19-B2C3-276A54491558}&end
2.Complete the whple adder and do some simulation. The adder can work at 1GHz at the worse case.
2019/12/18,
1.Solved the problems meeted in simulation and sumulated the adder at different frequency, the delay is worse than reults in the paper.
2.Wrote another adder using different equations to verify its function, debugging.
3.Scan some literatures about clock-delay domino.
2019/12/25
1.Complete the gate-level verilog of 64-bit Ling radix-2 sparse-2 adder with correct function.
2.Synthesis the adder mentioned above and a direct "a + b" adder, the direct "a + b" adder performs a little better than the adder mentioned above.
3.Read two papers.
2019/01/08
1.Read related documents about Dsign Compiler and Designware libirary, the CLA IP will be used by DC when write "a + b" in source file. 
2.Construct 64-bit Ling's adder using virtuoso.
3.Read papers:
  1.Reconfigurable Convolutional Kernels for Neural Networks on FPGAs(not understanding)
  2.Synetgy
  3.An Efficient Hardware Accelerator for Sparse Convolutional Neural Networks on FPGAs
  4.Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions
2019/01/15
1.Complete 64-bit Ling adder except sum generation logic.
2.Compare the latency with  previous 64-bit adder.
3.Read paper:
  a. Addernet
  b. REQ-Yolo
2020/02/12
1.Scan videos about Sequence model in deep lerning.
2.Read paper ”Fast Multiplier Generator for FPGAs with LUT based Partial Product Generation and Column/row Compression”, note will be uploaded to Github.
3.Read paper “Reconfigurable Constant Multiplication for FPGAs”, not very understanding, note will be uploaded next week.
4.Begin to read documents about LUT’s architecture of Xilinx’s 7 series FPGA.
2020/02/19
1.Read paper"RoCoCo: Row and Column Compression for High-Performance Multiplication on FPGAs".
2.Read a tutorial "Compute-Efficient Neural-Network Acceleration" on FPGA2019.
2020/02/26
1.Read paper "Reuse Kernels or Activations? A Flexible Dataflow for Low-latency Spectral CNN Acceleration" on FPGA2020
2020/03/04
1.Read paper from FPGA 2020 "Reuse Kernels or Activations? A Flexible Dataflow for Low-latency Spectral CNN Acceleration"
2.Read paper from ISSCC 2020 Session 14 "A 510nW, 0.41V Low-Memory, Low-Computation Keyword Spotting Chip using Serial FFT based MFCC and Binarized Depthwise Separable Convolutional Neural Network in 28nm CMOS", use its slides as reading note.
3.Find a bug in Ling adder constructed in Virtuso, debugging.
2020/03/11
1.Fix the bug found last week.
2.Reduce the pulse width of clock signal during pre-charge stage in order to increase the clock frequency(625ps now).
3.Read paper "DARB: A Density-Adaptive Regular-Block Pruning for Deep Neural Networks".
4.Read paper "A 65nm 24.7μJ/Frame 12.3mW Activation-Similarity-Aware Convolutional Neural Network Video Processor Using Hybrid Precision, Inter-Frame Data Reuse and Mixed-Bit-Width Difference-Frame Data Codec". Nor complete.
2020/03/18
1.Modify the size of transistor to improve the latancy of adder.
2.Compare the adder with the adder in paper and the adder synthesized by DC.
2020/03/25
1.Use custom inverter to replace the inverter from digital library in order to improve the latency of adder.
2.Count the number of MOSFET in the netlist got from Design Compiler.
2020/04/01
1.Read paper about variable latency speculative adder.
