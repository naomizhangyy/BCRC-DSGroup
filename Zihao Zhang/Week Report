2019/03/20
1.Finish Tanji3 DLA documents writing, include control module of LPE, control module of APE and the top module of control. The documents will be uploaded.
2.Study base 2 FFT algorithm that extracted by frequncy and base 4 FFT algorithm that extracted by time(base 4 FFT algorithm further reduce the computation complexity compared with base 2 FFT algorithm) in order to have a deeper understanding on Tingshua's Paper on ISSCC2019.
  a.Based on CirCNN
  b.Use FFT to compute convolution in CirCNN model
3.Read ISSCC2019 Paper "An 11.5TOPS/W 1024-MAC BUtterfly Structure Dual-Core Sparsity-Aware Neural Processing Unit in 8nm Flagship Mobile Soc":
  a.Moving Output Feature Maps to compute convolution
  b.Butterfly-Structure Dual-Core Accelerator
  c.Sparsity-Aware Computing:Using Feature Selection
4.Prepare the slide about accelerator based on Winograd Convolution.The Slides Will be uploaded.
2019/03/28
1.Read Paper "FPGA based efficient on-chip memory for image processing algorithms" to learn several main data access policies for image processing algorithms.
  a.Prposed a sub-bank Dual Port memory architecture based on Single Port SRAM which supports several main data access policies.
  b.Confused with its clock generation block and memory control unit.
2.Read Tingshua's Paper on ISSCC 2019, got its main idea about dataflow and methods to  efficiently do RFFT(The inputs are real numbers):
  a.Use CirCNN model proposed on Micro 2017.
  b.Using radix-2 DIT(decimation in time) algorithm and use 7-stage butterfly architecture and each stage contains 32 basic butterfly units to support up to 128-point FFT.
  c.Using "doubling algorithm" to avoid redundant operations in CFFT computing.
  But there are some details that I don't understand clearly:
  a.Why the weight matrixes are block-circulant matrixes after rearranging?
  b.Details about how to realize 1-to-12b data-width supporting.
  c.Details about HBST-TRAM.
  I will do a representation when I completely understand this paper.
3.Read Paper "A Pipelined FFT Architecture for Real-Valued Signals" to learn some architectures for real-valued signals.
4.Modify the slides of "Winograd Convolution", the representation will da tonight.
2019/04/04
1.Attend a lecture about accelerating ddep convolutional network inferences at algorithm level given by Bei Yu from CUHK on Friday afternoon.
2.Scan KAIST's ISSCC Paper about LNPU, an atchitecture aiming to accelerate DNN's training at edge. Need to study knowledge about training for further understanding.
  a.Use mixed precision(FP8 and FP16) to improve enegy efficiency with slight accuracy loss.
  b.Support zero-skipping operation in computing core.
3.Learn speech given by Bert Moons on ISSCC 2019, the topic is "Enabling Embedded Intelligence: Application, architecture and design solutions".
4.Ask for leave from Tuesday to Thursday.
2019/04/11
1.Study and understand some details of Tingshua's ISSCC2019 Paper which confused me during previous reading:
  a.Understand the method which makes the weight matrix after reformulation still be a block-circulant matrix
  b.The data-resue pattern during element-wise multiplication
  c.Serial-to-parallel transformation during dataflow
2.Find some new details that don't appear in slides and paper
  a.Because some twiddle factors can't be represented precisely，so FFT may bring error.
  b.The method of rounding between stages.
3.Write a block to do 64-point fft using radix-2 DIT algorithm for further research and modification. Not complete.
2019/04/18
1.Read paper "CircConv: A Structured Convolution with Low Complexity":
  a.Use circulant tensor for weight(the same as CirCNN does I think but use another method to explain the structured tensor)
  b.Propose a method to training circulant CNN from a pre-trained model
2.Read paper "Optimizing Bit-Serial Matrix Multiplication for Reconfigurable Computing":
  a.Intoduce a framework named "BISMO" to dp bit-serial matrix multiplication in order to support variable-precision computation proposed in their previous work
  b.Modify original Dot Product Unit(DPU) with matirx compression techniques to decrease the height of the input data at the cost of width increation.
  c.Modify the shifter in DPU by change the computation order so that the new shifter only need to support one type(shift left by one bit) of shift opertion
  d.Proposed bit-parallel to bit-serial transformaiton to reshape the data stored in main memory for efficient access.
  e.The software of "BISMO" framework still need to study.
3.Study math about matrix computation to find some tecniques to accelerate matrix computation(use the book writen by Gene H.Golub and Charles F.Van Loan)
2019/04/25
1.Think how to eliminate redundant operations during transformation of input feature map tile in Winograd convolution.
  a.Because there exists overlapping between neighboring input feature map tiles
2.Think how to use an unified computing architecture to support both complex-number matrix multiplication and real-number matrix multiplication.
3.Think if there exists an unified architecture supporting both radix-2 FFT and radix-4 FFT
2019/05/09
1.Read Paper about PERMDNN, which uses structured matrix-based approach like CirCNN to bring regularity to sparse  weight matrix.
  a.Make the weight matrices of the DNN model consist of multiple permuted diagnal sub-matrices.
  b.Permuted diagnal matrix require less storage than unstructured matrix becasue its its row index can be got by its column index.
  c.More flexibility than CirCNN because PERMCNN dose not have any restrictions on the size of block permuted diagnal matrix.
  d.Avoid computation between complex numbers
2.Read Paper about how to pack sparse CNNs for efficient systolic array implementations
  a.Use column combining to make sparse weight matrix become dense.
  b.Don't know how to combine the correspondingrows of input feature map matrix.
  c.Use bit-serial MAC.
2019/05/16
1.Read paper "An N-Way Group Association Architecture and Sparse Data Group Association Load Balancing Algorithm for Sparse CNN Accelerators" by the team of STICKER.
2.Think about if it is effective to store weights as Booth code and use Booth multiplication during computation of CNNs.
2019/05/25
Refer to Month Report-May.docx
2019/05/30
1.Read Paper "Architecture of a programmable systolic array"
  a.Streaming instructions like data
2.Think how to transfer pooling operation  to Winograd domain
2019/06/13
1.Read some papers about architectures of DNNs and types of convolution:
  a.HetConv: use heterogeneous convolution to replace homogeneous convolution, heterogeneous kernel means the size of weight matrices are different through input channels.
  b.DetNet: a backbone network for object detection based-on ResNet.
  c:Dialated covolution: use dialated convolution to replace pooling operation in order to avoid loss of spatial resolution.
2.Read paper "Partial-product Generation and Addition for Multiplication in FPGAs With 6-Input LUTs":
  a.Propose a booth architecture according to the features of CLB in Xilinx FPGA.
3.Read paper of DAC'2019:
  a.bitblade: modified the computation sequence of Bitfusion and acquire area and power reduction.
  b.Layer conscious memory management: solve the data transfer bottlenecks in FPGA-based DNN accelerator according to the shape of concrete layer.
2019/06/13
1.Do the work of assistant.
2.Think an idea about new DLA that sharing the partial production generation component of Booth multiplier during massive MAC operations.
2019/06/27
1.Prepare for the medium term report.
2.Do the work of assistant.
2019/07/04
Previous work this year:
1.Help analog group modify and verify a module.
2.Read papers about network architecture and DNN accelerator.
3.Learn FFT and study its combination with convolution computing.
4.Try to find some sparse matrices which are different from CirCNN and PermDNN.
Next Plan:
1.Study network training for future work.
2.Design a deep learning accelerator which supports 2-bit, 4-bit, 8-bit weight and 8-bit weight.
3.Complete the paper.
2019/07/11
1.Learn some light network architectures such as MobileNets and ShuffleNets. 
2.Think how to efficiently support some novel convolution architectures such as depthwise seperable convolution and so on.
  a.Read papers such as Eyeriss_v2 and MAERI(ASPLOS'18) which support flexible dataflow.
3.Think the soc design of the system for the final demo and discuss some details such as data transfer with TaoFeng.
2019/08/08
1.Talked with professor.
2.Took part in Chisel Community Conference 2019 on Saturday and Sunday to learning something about Chisel, a kind of HCL.
3.Learn someting about AXI4 Stream protocol and corresponding IP provided by Xilinx.
4.Think some detalis in paper "Deep Convolution Neural Network Architecture With Reconfigurable Computation Patterns"
To do list:
1.Continue working with the new idea, think how to reduce the resource consumption for encoding and partial product producing.
2.Continue learn something about network training.
3.Read paper that received from professor. Learn something about BERT.
2019/08/15
1.Help Jingsen debug the demo.
2.Study netwok traing based on MXNet.
3.Have an idea about pre-computation.(some additions can be completed before do inference on the chip).
2019/08/22
1.Learn feature extraction in Speech recognition and Synthesis.
2.Study details of Wavenet.
3.Train a very simple network for multi-classification using mxnet.
2019/08/29
1.Read code about implementation of Wavenet.
2.Read paper how to decrease the time complexity of computation in Wavenet according to pre-computation.
3.Learn some basic knowledge about general audio related tasks.
2019/08/29
1.Continue study details of original wavenet and fast wavenet.
2.Study how to use the DMA in Zynq Processing System.
2019/09/11
1.Finished code reading and made slides to illustrate the details about wavenet. 
2019/0925
1.Participated "The Industrial Fair" on Friday and Saturday.
2.Discussed with professor on Sunday.
3.Raed paper ”Rethinking floating point for deep learning” and related literature
2019/10/17
1.Completed the train of Tacotron2 and run some cases on trained Tacotron2 followed by official Waveglow.
2.made some graphs to show the data distribution of mel-spectrum generated by Tacotron2 and audio signal generated by official Waveglow.
3.Began tp train Waveglow using 4 GPUs.
4.Did some research about multi-operands adder.
2019/10/23
1.Read TingsHua's paper about accelerator about speech synthesis.
2.Think the architecture of the adder used to add a lot of fractions from posit floating point numbers togather.
2019/10/30
1.Writing patent.
2.Read paper about adders.
2019/11/06
1.Modify the patent.
2.Write 256-bit Carry Skip Adder according to a paper.
2019/11/13
1.Finished the Carry skip Addder writing and did some simply analysis and synthesis
2.Raed the paper about adder of Berkely and related papers.
To do list:
1.Use custom disign methodology to construct an adder.
2019/11/20
1.Study Ling's addition algorithm.
2.Study some tree architecture like kogge-stone and Ladner–Fischer, both dense and sparse.
2.Review "Digital Integrated Circuits".
